
\chapter{Introduction}

\section{Background}

Parallel computing has drawn increasing attention in the field of high-performance computing.
Today, it is widely accepted that Moore's law will be broken down due to physical constraints, and future performance increase must heavily rely on increasing the number of cores, rather than making a single core run faster. 

Typically, a parallel computation can be completed by splitting it into multiple subcomputations executing independently.
The theoretical maximum number of these subcomputations is called the parallel degree. 
In practice, it is common that this theoretical parallel degree cannot be achieved because of the limitation of physical resources.

Parallelism can be expressed or employed in different aspects, such as algorithms, programming languages or hardware, which all have made tremendous progress in recent decades.


\section{Nested data parallelism}

Data parallelism focuses on expressing parallelism from the perspective of programming languages.
The observation is that the loop structure occurs quite frequently in algorithms and usually accounts for a large proportion of the running time, which has a high potential for each iteration being executed in parallel.
 
Nested data parallelism (NDP) places emphasis on parallel computations on nested data structures.
In nested data-parallel languages, function calls can be applied to a set of data that can be not only flat or one-dimension arrays, but also multi-dimension, irregular or recursive structures. 
So these languages can implement parallel algorithms, including nested loops and recursive ones, more expressively and closer to the programmer's intuition.

On the other hand, at such a high level, compilation becomes more complicated to achieve a performance close to the code written directly in low-level ones. 
Also, predicting the performance are more difficult because the details of the concrete parallel execution are usually implicit or hidden to the programmer.

Some languages, such as NESL \cite{blel95nesl,Blel-imp,Blel96-proof} and Proteus \cite{prins93,Ppw95-workeff}, have pioneered NDP significantly and demonstrated its advantages and importance.


\section{NESL}
NESL is a first-order functional nested data-parallel language. The main construct to express data-parallelism in NESL is called \emph{apply-to-each}, whose form, shown below, looks like a mathematical set comprehension:
$$\{e_0 : x \ \*{in} \ e_1 \} $$
As its name implies, it applies the function $e_0$ to each element of $e_1$, and the computation will be executed in parallel.
As an example, adding 1 to each element of a sequence $[1,2,3]$ can be written as the following apply-to-each expression:
$$ \{x+1 : x \ \*{in} \ [1,2,3] \}$$

The strength of this construct is that it supports applying functions on irregular, nested data sets with guarantee of random access. 
Continuing with the example above, the adding-1 operation can also be performed to each subsequence of a nested sequence [[1,2,3],[4],[5,6]], written as: 
$$\{\{y+1: y \ \*{in} \ x\}: x \ \*{in} \ [[1,2,3],[4],[5,6]]\}$$
and if we want to compute the sum of each subsequence, we can write:
$$\{\*{sum}(x) : x \ \*{in} \ [[1,2,3],[4],[5,6]]\}$$



The low-level language of NESL's implementation model is VCODE \cite{Blel-imp}, which uses vectors (i.e., flat arrays of atomic values such as integers or booleans) as the primitive type, and its instruction set performs operations on vectors as a whole, such as summing. 
The technique $ flattening \ nested \ parallelism$ \cite{Blel-flatten} used in the implementation model translates nested function calls in NESL to instructions on vectors in VCODE. \\


From the user's perspective, the first highlight of NESL is that the design of this language makes it easy to write readable parallel algorithms. The apply-to-each construct is more expressive in its general form:
$$ \{ e_1 :  x_1 \ \*{in} \ seq_1 \ ; \ ... \ ; \ x_i \  \*{in} \ seq_i \ \ | \ e_2  \}$$
where the variables $x_1, ...,x_i$ possibly occurring in $e_1$ and $e_2$ are corresponding elements of $seq_1,...,seq_i$ respectively; $e_2$, called a $sieve$, performs as a condition to filter out some elements. 
Also, NESL's built-in primitive functions, such as scan \cite{blel89scan}, are powerful for manipulating sequences.
An example program of NESL for splitting a string into words is shown in Figure~\ref{fig-nesl-wordpart}.

\begin{figure}[H]
\lstinputlisting[style=nesl-style]{code/wordpart.nesl} 
\begin{lstlisting}[style=nesl-style]
-- a running example
$> str2wds("A   NESL program . ")
[['A'], ['N', 'E', 'S', 'L'], ['p', 'r', 'o', 'g', 'r', 'a', 'm'], ['.']] :: [[char]]
\end{lstlisting}
\caption{{A NESL program for splitting a string into words \label{fig-nesl-wordpart}}}
\end{figure}


Another important idea of NESL is its intuitive, language-based, high-level cost model \cite{blel96cost}.
This cost model can predict the performance of a NESL program from two aspects: $work$, the total number of operations executed in this program, and $depth$, or $step$, the longest chain of sequential dependency in this program.
From another point of view, the work cost can be viewed as a measurement of the time complexity of the computation executed on a machine with only one processor,
and the step cost corresponds to the time complexity on an ideal machine with an unlimited number of processors. 

With this work-depth model, as demonstrated in \cite{Blel90vecmod},   the user can derive the asymptotic time complexity $T$ of a NESL program executed on a machine with $P$ processor as 
$$ T = O(work/P + depth) $$ \\


However, a problem NESL suffers from is its inefficient space-usage.
This is mainly due to the eager semantics that NESL uses for supporting random access in parallel execution.
That is, during the execution of a NESL program, at least the space for storing the entire input/output data must be provided.
For example, computing the multiplication of two matrices of size $n$-by-$n$ will need at least $n^3$ space to store the intermediate element-wise multiplication result, which is a huge waste of space when $n$ is large.

\section{SNESL}
Streaming NESL (SNESL) \cite{Fphd} is a refinement of NESL
that attempts to improve the efficiency of space usage. 
It extends NESL with two features: streaming semantics and a cost model for space usage. 
The basic idea behind the streaming semantics may be described as:
data-parallelism can be realized not only in terms of space, as NESL has demonstrated, but also, for some restricted cases, in terms of time. 
When there is no enough space to store all the data at the same time, computing them chunk by chunk may be a way out.
This idea is similar to the concept of $piecewise \ execution$ in \cite{palm95}, but SNESL makes the chunking exposed at the source level in the type system and the cost model instead of a low-level execution optimization.

\subsection{Types}

The types of a minimalistic version of SNESL defined in \cite{Fphd} are as follows:
\begin{align*} 
& \pi ::= \bool \ | \ \int \ | \ ...  \\
& \tau ::= \pi \ | \ (\tau_1,...,\tau_k) \ | \ [\tau]  \\
& \sigma ::= \tau \ | \ (\sigma_1,...,\sigma_k) \ | \ \tseq{\sigma}  
\end{align*}
Here $\pi$ stands for the primitive types and $\tau$ the concrete types, both originally supported in NESL.
The type $[\tau]$, which is called $sequences$ in NESL and $vectors$ in SNESL, represents spatial collections of homogeneous data, and must be
fully allocated or $materialized$ in memory at once for random access.  
$(\tau_1,...,\tau_k)$ are tuples with $k$ components that may be of different types.

The novel extension is the $streamable$ types $\sigma$, which generalizes the types of data that are not necessarily entirely materialized at once, but rather in a streaming fashion. 
In particular, the type $\{\sigma\}$, called $sequences$ in SNESL, represents collections of data computed in terms of time, which means the elements of a sequence are collected over time.
So, even with a small size of memory, SNESL could execute programs which are impossible in NESL due to space limitation or more space efficiently than in NESL. 

For clarity, from now on, we will use the terms consistent with SNESL.

\subsection{Values and expressions}

The values of SNESL are as follows:
\begin{align*}
& a ::=  \T \ | \ \F \ | \ n \ (n \in \mathbb{Z}) \ | \ ... \\
& v ::=  a \ | \ (v_1,...,v_k) \ | \ [v_1,...,v_l] \ | \ \{v_1,...,v_l\} 
\end{align*}
where $a$ is the atomic values or constants of types $\pi$, and $v$ are
 general values which can be a constant, a tuple of $k$ components, a vector or a sequence of $l$ elements. 
Here, as part of our notation, we use $k$ to range over small natural numbers, while $l$ are potentially large ones. 

The expressions of SNESL are shown in the following figure.

\begin{figure}[H]\large 
\begin{alignat*}{2}
& e &&::=  a \     \tag{constant} \\
&   && \quad | \ x  \tag{variable} \\
&   && \quad | \ (e_1,...,e_k) \tag{tuple}\\
&   && \quad | \ \Let{x}{e_1}{e_2} \tag{let-binding}\\
&   && \quad | \ \hcall{\Tupk{e}}  \tag{built-in function call} \\
&   && \quad | \ \{e_1: \ x \ \*{in} \ e_0 \} \tag{general comprehension} \\
&   && \quad | \ \{e_1 \ | \ e_0 \}\tag {restricted comprehension} 
\end{alignat*}
\caption{Syntax of SNESL expressions \label{fig-snesl-exps}}
\end{figure}

As an extension of NESL, SNESL keeps a similar programming style of NESL. 
Basic expressions, such as the first five in Figure~\ref{fig-snesl-exps}, are the same as they are in NESL. 
The apply-to-each construct in its general form splits into the general and the restricted comprehensions:
the general one now is only responsible for expressing parallel computation,
and the restricted one can decide if a computation is necessary or not, working as the only conditional in SNESL.
Also, these comprehensions extend the semantics of the apply-to-each from evaluating to vectors (i.e., type $[\tau]$) to evaluating to sequences (i.e., type $\{\sigma\}$). 
A notable difference between them is that the free variables of $e_1$ in the general comprehension can only be of concrete types, while they can be of any types in the restricted one.

Note that SNESL, as described in \cite{Fphd}, does not include programmer-defined functions. 
In the implementation, functions can be defined, but effectively treated as macros during compilation.
In particular, they cannot be recursive.

\subsection{Primitive functions}

SNESL also refines the primitive functions of NESL to separate sequences and vectors.
The primitive functions of SNESL are shown in Figure~\ref{fig-snesl-func}.

\begin{figure}[H]\large
\begin{alignat}{2} 
&\hcall && ::= \oplus \ | \ \ \*{append} \ | \ \*{concat} \ | \ \*{zip} \ | \ \*{iota}  \ | \ \*{part}  \ | \ \*{scan}_{\otimes} \ | \ \*{reduce}_{\otimes} \ | \ \*{mkseq} \\
&   && \quad | \ \*{length} \ | \ \*{elt} \\
&   && \quad | \ \*{the}  \ | \ \*{empty} \\
&   && \quad | \ \*{seq} \ | \ \*{tab} \\
& \oplus  \ && :: = + \ | \ - \ | \ \times \ |  \  / \ | \ \% \ | ==  \ |\ <=  \ | \ \*{not} \ | \ ... \tag{scalar operations} \\
& \otimes \ && :: = + \ | \ \times  \ | \ ...  \tag{associative binary operations}
\end{alignat}
\caption{SNESL primitive functions \label{fig-snesl-func}}
\end{figure}

The scalar functions of $\oplus$ and $\otimes$ should be self-explanatory from their conventional symbols. 
The types of the other functions and their brief descriptions are given in Table~\ref{tab:snesl-funcs}.

The functions listed in (1.1) and (1.2) of Figure~\ref{fig-snesl-func} are original supported in NESL, doing transformations on scalars and vectors.
In SNESL, list (1.1) are adapted to streaming versions with slight changes of parameter types where necessary.
By streaming version we mean that these functions in SNESL take sequences as parameters instead of vectors as they do in NESL, as we can see from Table~\ref{tab:snesl-funcs}, thus most of these functions can execute in a more space-efficient way.  

Functions in (1.2), i.e., $\*{length}$ and $\*{elt}$, are kept their vector versions in SNESL. 
These two exploit vectors that are fully materialized, thus have constant time cost.
On the other hand, while analogous functions can be defined for sequences (using the other primitives), they have cost proportional to the length of the sequence.

List (1.3) are new primitives in SNESL.  
The function $\*{the}$, returning the sole element of a singleton sequence, can be used to simulate an if-then-else expression together with restricted comprehensions:
$$\*{if} \ e_0 \ \*{then} \ e_1 \ \*{else} \ e_2 \equiv \*{the}(\{e_1 \ | \ e_0 \} {\++} \{e_2 \ | \ \*{not}(e_0)\}) $$
The function $\*{empty}$, which tests whether a sequence is empty or not, only needs to check at most one element of the sequence instead of materializing all the elements. Therefore, it works in a fairly efficient way with a constant complexity both in time and space.

Finally, functions listed in (1.4) connects the concrete types and streams, making it possible to turn every NESL program into a SNESL one by adding suitable $\*{seq}/\*{tab}$ to executed in a streaming fashion. 

\begin{table}\large
	\renewcommand\arraystretch{1.5}
	\centering
	\begin{tabular}{|p{0.4\columnwidth}|p{0.5\columnwidth}|}
		\hline
		Function type & Brief description  \\ \hline
		$\*{append : \{\sgm\} \times \{\sgm\} \ra \{\sgm\}}$ & append two sequences; syntactic sugar: infix symbol ``${\++}$" \\ \hline
		$\*{concat: \{\{\sgm\}\} \rightarrow \{\sgm\} }$ &  flatten a sequence of sequences                 \\ \hline
		$\*{zip}: \{\sgm_1\} \times ... \times \{\sgm_k\}  \ra \{(\sgm_1,...,\sgm_k)\}$ & convert $k$ sequences into a sequence of $k$-component tuple \\ \hline
		$\*{iota: \int \ra \{\int\}}$  &  generate an integer sequence starting from 0 to the given argument minus one; syntactic sugar: symbol ``\&"    \\ \hline
		$\*{part: \{\sgm\} \times \{\bool\} \ra  \{\{\sgm\}\}}$   & partition a sequence into subsequences segmented by $\T$s in the second argument; e.g., $\*{part}(\{3,1,4\}, \{\F,\F,\T,\F,\T,\T\}) = \{\{3,1\},\{4\},\{\}\}$                \\ \hline
		$\*{scan_\otimes}: \{\int\} \ra \{\int\}$     &  performs an exclusive scan of $\otimes$ operation on the given sequence.    \\ \hline
		$\*{reduce_\otimes: \{\int\} \ra \int }$     &   performs a reduction of $\otimes$ operation on the given sequence             \\ \hline
		$\*{mkseq}: (\overbrace{\sgm,...,\sgm}^{k}) \ra \{\sgm\}$  & make a $k$-component tuple to a sequence of length $k$ \\ \hline  
	    $\*{length}$: $[\tau] \ra \int$ & return the length of a vector; syntactic sugar: symbol ``\#" \\ \hline  
	    $\*{elt}$: $[\tau] \times \int \ra \tau$  & return the  element of a vector with the given index; syntactic sugar: infix symbol ``!" \\ \hline  
	    $\*{the:  \{\sgm\} \ra \sgm}$     &     return the element of a singleton sequence           \\ \hline
	    $\*{empty:  \{\sgm\} \ra \bool}$       & test if the given sequence is empty.             \\ \hline  
	    $\*{seq}: [\tau] \ra \{\tau\} $  & convert a vector into a sequence \\ \hline  
	    $\*{tab}: \{\tau\} \ra [\tau] $  & tabulate a sequence into a vector\\ \hline  
	\end{tabular}
	\caption{SNESL primitive functions}
	\label{tab:snesl-funcs}
\end{table}




The SNESL program for string splitting is shown in Figure~\ref{fig-snesl-wordpart}. 
Compared with the NESL counterpart in Figure~\ref{fig-nesl-wordpart}, the code of SNESL version is simpler, because SNESL's primitives make it good at streaming text processing. 
In particular, this SNESL version can be executed with even one element space.
 
\begin{figure}[H]
	\lstinputlisting[style=nesl-style]{code/wordpart.snesl} 
	\caption{{A SNESL program for splitting a string into words \label{fig-snesl-wordpart}}}
\end{figure}


\subsection{Cost model}

Based on the work-depth model, SNESL develops another two components for estimating the space complexity \cite{MadFil13}. 
The first one is the sequential space $S_1$, that is, the minimal space to perform the computation, corresponding to run the program with a buffer of size one. 
In \cite{Fphd}, this component is refined further to take the vector (which needs to be fully materialized anyway) into account.
The another is the parallel space $S_\infty$, the space that needed to achieve the maximal parallel degree, and it corresponds to assuming the program executes with an unlimited memory as NESL does.

With this extended cost model, the user can now estimate the time complexity of a SNESL program using the derivation same as the one for NESL, and the space complexity with the following formula
$$ S = O(min(P \cdot S_1, S_\infty)) $$
where $P$ is the number of processors.
	
%\section{Mathematical background and notations}
%\begin{itemize}
%	\item Set difference: \\
%	For two sets $A$ and $B$, 
%	\[ A \ \backslash \ B = \{s | s \in A \wedge s \notin B\} \]
%	
%	It is easy to prove the following properties: 
% 	\begin{itemize}
%	 	\item For any three sets $A,B$ and $C$: 
%	 			\eq{set-p1}{(A \bs B) \cap C = (A \cap C) \bs B  =  A \cap (C \bs B)}
%	 	\item For two sets $A$ and $B$,
%	 	       \eq {set-p2}{A \cap B = \emptyset  \Leftrightarrow A \bs B = A}
% 	\end{itemize}
%
%\end{itemize}